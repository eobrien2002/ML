{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "During this project I will use the Keras API to build a deep learning translator. I preprocessed the data to get one-hot encoded vectors, then train the model using LTSM and Dense from Keras, and finally test the model using various input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our translations\n",
    "data_path = \"fra.txt\"\n",
    "\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "# Building empty vocabulary sets\n",
    "input_tokens = set()\n",
    "target_tokens = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines[:10000]:\n",
    "  # Input and target sentences are separated by tabs\n",
    "  input_doc, target_doc = line.split('\\t')[:2]\n",
    "\n",
    "  # Appending each input sentence to input_docs\n",
    "  input_docs.append(input_doc)\n",
    "  \n",
    "  #The below expression tokenizes the target_doc into a list of words and punctuation marks, \n",
    "    # and then concatenates them into a single string separated by spaces.\n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  # Redefine target_doc below\n",
    "  # and append it to target_docs:\n",
    "  target_doc = '<START> ' + target_doc + ' <END>'\n",
    "  target_docs.append(target_doc)\n",
    "\n",
    "  # Now we split up each sentence into words\n",
    "  # and add each unique word to our vocabulary set\n",
    "  #The below expression tokenizes the input_doc into a list of words and punctuation marks, \n",
    "  # and iterates over each token\n",
    "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "    # print(token)\n",
    "    # Add your code here:\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "  for token in target_doc.split():\n",
    "    # print(token)\n",
    "    # And here:\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create num_encoder_tokens and num_decoder_tokens:\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "#this code calculates the length of the longest sequence of tokens in the input_docs list by tokenizing \n",
    "# each input_doc into a list of tokens and finding the maximum length of these lists\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to create dictionaries of each word for the input (English) and output (Japanese). We also need \n",
    "#reverse dictionaries so that we can find the word based on the index\n",
    "\n",
    "input_features_dict = dict([(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict([(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_input_features_dict = dict((i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict((i, token) for token, i in target_features_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to create numpy arrays with 0s. THe arrays will be filled with a 1 for the token that we are looking to encode and decode.\n",
    "# This is because the keras model requires all words to be in one-hot encode vectors\n",
    "encoder_input_data = np.zeros((len(input_docs), max_encoder_seq_length, num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_docs), max_decoder_seq_length, num_decoder_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "    #This loop processes the input data. For each line and each token (word or punctuation) in the input document, \n",
    "    # the code finds the index of the token in the input feature dictionary (input_features_dict). \n",
    "    # This index is used to set the corresponding entry in the 3D encoder input data array (encoder_input_data) to 1. \n",
    "    # This creates a one-hot encoding of the input data.\n",
    "  for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "    # Assign 1. for the current line, timestep, & word\n",
    "    # in encoder_input_data:\n",
    "    encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "\n",
    "  for timestep, token in enumerate(target_doc.split()):\n",
    "\n",
    "    decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "    if timestep > 0:\n",
    "\n",
    "      decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '$', '%', '&', ',', '-', '.', '17', '19', '2', '3', '30', '5', '50', '7', '8', '99', ':', '?', 'A', 'Abandon', 'Act', 'Add', 'After', 'Aim', \"Ain't\", 'Air', 'All', 'Allow', 'Am', 'American', 'Answer', 'Any', 'Anybody', 'Anyone', 'Anything', 'Apples', 'Arabs', 'Are', \"Aren't\", 'Arm', 'Asian', 'Ask', 'Attack', 'Autumn', 'Avoid', 'Awesome', 'B', 'Back']\n"
     ]
    }
   ],
   "source": [
    "print(list(input_features_dict.keys())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annulez\n"
     ]
    }
   ],
   "source": [
    "print(reverse_target_features_dict[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2269\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the number of dimensions of the internal representation \n",
    "# of the input sequences in the encoder LSTM and decoder LSTM layers of a neural network.\n",
    "latent_dim = 256\n",
    "#The Batch Size is a hyperparameter of a machine learning model that defines the number of samples to work \n",
    "# through before updating the internal model parameters.\n",
    "batch_size = 75\n",
    "#Epoch specifies the number of times the training loop will run over the entire training data.\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below code creates an encoder network for a seq2seq model using Keras.\n",
    "\n",
    "#This defines an input layer for the encoder with a shape of (None, num_encoder_tokens), where None represents the \n",
    "# length of the sequence and num_encoder_tokens is the number of tokens/words in the encoder vocabulary.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "#This creates an LSTM layer with latent_dim number of units, and with return_state set to True, \n",
    "# which means it will return the hidden state and cell state of the LSTM layer in addition to its outputs.\n",
    "#The return_state=True in the LSTM layer is important because the hidden state and cell state are needed for the decoder \n",
    "# part of the model to properly predict the output sequence. The hidden state and cell state capture the context information \n",
    "# from the input sequence that is fed into the encoder. This information is then used by the decoder to generate the target sequence.\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "#The third line applies the encoder_inputs to the encoder_lstm layer, and the outputs, state_hidden, and state_cell are assigned \n",
    "# to encoder_outputs, state_hidden, and state_cell respectively.\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "\n",
    "#This defines a list called encoder_states, which includes the hidden and cell states of the LSTM layer. \n",
    "# This will be used as the initial state of the decoder network.\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is defining and implementing an LSTM decoder in Keras, which is used in a seq2seq neural network.\n",
    "\n",
    "\n",
    "#The first line creates an input layer for the decoder with shape (None, num_decoder_tokens), where \"None\" indicates \n",
    "# a variable-length sequence and \"num_decoder_tokens\" is the number of tokens in the decoder's vocabulary. The \"None\" in \n",
    "# the shape of the decoder_inputs layer means that the length of the input sequences can vary and it is not fixed. \n",
    "# This allows the decoder to handle inputs of different lengths, which is important because \n",
    "# the length of the source and target sentences can be very different\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "#The second line creates an LSTM layer with \"latent_dim\" units and specifies\n",
    "#  to return both the sequence of outputs and the hidden and cell states.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "#The third line applies the LSTM layer to the decoder_inputs and initializes the hidden and cell states with the encoder_states.\n",
    "#  It also splits the outputs and states into separate variables.\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "#The fourth line creates a dense layer with \"num_decoder_tokens\" units \n",
    "# and a \"softmax\" activation function, which will be used to produce a probability distribution over the decoder's vocabulary.\n",
    "#A dense layer is a type of layer in a neural network that has connections to all the neurons in the previous layer. \n",
    "# Each neuron in the dense layer receives input from every neuron in the previous layer. The dense layer performs a \n",
    "# matrix multiplication of the input with a weight matrix and adds a bias term to produce the output. \n",
    "# The activation function applied to the output then determines the final activation values for each neuron in the dense layer.\n",
    "#The softmax function maps its input to a probability distribution over the classes, with each output value representing the predicted probability\n",
    "# for each class. The softmax function normalizes the input so that the sum of all the outputs is equal to 1, which represents a valid \n",
    "# probability distribution. The class with the highest probability is chosen as the final prediction. \n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "#he fifth line applies the dense layer to the decoder_outputs to obtain the final decoder output.\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the training model:\n",
    "#By creating a model in this way, it is now possible to train the network on data by passing input \n",
    "# sequences to the encoder and decoder and using the decoder outputs as the target during training. \n",
    "# The model can be compiled, fit to data, and used to make predictions, which can be compared to the target to compute the \n",
    "# training loss and update the model's weights.\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "\n",
      "Model: \"model_29\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_43 (InputLayer)          [(None, None, 2269)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " input_44 (InputLayer)          [(None, None, 4488)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " lstm_26 (LSTM)                 [(None, 256),        2586624     ['input_43[0][0]']               \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_27 (LSTM)                 [(None, None, 256),  4858880     ['input_44[0][0]',               \n",
      "                                 (None, 256),                     'lstm_26[0][1]',                \n",
      "                                 (None, 256)]                     'lstm_26[0][2]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, None, 4488)   1153416     ['lstm_27[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,598,920\n",
      "Trainable params: 8,598,920\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model summary:\\n\")\n",
    "training_model.summary()\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model:\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Categorical cross-entropy is a loss function used for multi-class classification problems. It is used to measure the \n",
    "# difference between the predicted probability distribution and the true distribution of the target classes. \n",
    "# The output of the loss function is a scalar value that summarizes the average discrepancy between the predicted \n",
    "# class probabilities and the true class labels for the given input data.\n",
    "\n",
    "#RMSprop (Root Mean Square Propagation) is a popular optimization algorithm used in deep learning. \n",
    "# It is a gradient descent optimization algorithm that adapts the learning rates of individual parameters \n",
    "# based on the historical gradient information. The idea behind RMSprop is to divide the learning rate for each parameter \n",
    "# by a running average of the historical magnitudes of the gradients for that parameter, effectively reducing the \n",
    "# learning rate for parameters that have consistently high gradients. This can help prevent oscillations or divergence \n",
    "# during training and lead to faster convergence. RMSprop is often used with deep neural networks, especially \n",
    "# recurrent neural networks and convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "107/107 [==============================] - 105s 920ms/step - loss: 1.9115 - accuracy: 0.0643 - val_loss: 1.7605 - val_accuracy: 0.0691\n",
      "Epoch 2/50\n",
      "107/107 [==============================] - 89s 836ms/step - loss: 1.5058 - accuracy: 0.0861 - val_loss: 1.7177 - val_accuracy: 0.0743\n",
      "Epoch 3/50\n",
      "107/107 [==============================] - 88s 826ms/step - loss: 1.4550 - accuracy: 0.0914 - val_loss: 1.6558 - val_accuracy: 0.0853\n",
      "Epoch 4/50\n",
      "107/107 [==============================] - 86s 809ms/step - loss: 1.4246 - accuracy: 0.0932 - val_loss: 1.6631 - val_accuracy: 0.0747\n",
      "Epoch 5/50\n",
      "107/107 [==============================] - 86s 803ms/step - loss: 1.4015 - accuracy: 0.0945 - val_loss: 1.6101 - val_accuracy: 0.0848\n",
      "Epoch 6/50\n",
      "107/107 [==============================] - 78s 727ms/step - loss: 1.3803 - accuracy: 0.0955 - val_loss: 1.5978 - val_accuracy: 0.0814\n",
      "Epoch 7/50\n",
      "107/107 [==============================] - 87s 820ms/step - loss: 1.3583 - accuracy: 0.0984 - val_loss: 1.5869 - val_accuracy: 0.0840\n",
      "Epoch 8/50\n",
      "107/107 [==============================] - 87s 813ms/step - loss: 1.3321 - accuracy: 0.1048 - val_loss: 1.6323 - val_accuracy: 0.0823\n",
      "Epoch 9/50\n",
      "107/107 [==============================] - 90s 839ms/step - loss: 1.2983 - accuracy: 0.1124 - val_loss: 1.5034 - val_accuracy: 0.0964\n",
      "Epoch 10/50\n",
      "107/107 [==============================] - 88s 821ms/step - loss: 1.2608 - accuracy: 0.1181 - val_loss: 1.4780 - val_accuracy: 0.1096\n",
      "Epoch 11/50\n",
      "107/107 [==============================] - 88s 819ms/step - loss: 1.2255 - accuracy: 0.1241 - val_loss: 1.4486 - val_accuracy: 0.1131\n",
      "Epoch 12/50\n",
      "107/107 [==============================] - 84s 784ms/step - loss: 1.1955 - accuracy: 0.1303 - val_loss: 1.4206 - val_accuracy: 0.1258\n",
      "Epoch 13/50\n",
      "107/107 [==============================] - 79s 736ms/step - loss: 1.1640 - accuracy: 0.1353 - val_loss: 1.3947 - val_accuracy: 0.1284\n",
      "Epoch 14/50\n",
      "107/107 [==============================] - 84s 786ms/step - loss: 1.1336 - accuracy: 0.1395 - val_loss: 1.3669 - val_accuracy: 0.1307\n",
      "Epoch 15/50\n",
      "107/107 [==============================] - 96s 895ms/step - loss: 1.1061 - accuracy: 0.1429 - val_loss: 1.3406 - val_accuracy: 0.1377\n",
      "Epoch 16/50\n",
      "107/107 [==============================] - 85s 791ms/step - loss: 1.0791 - accuracy: 0.1470 - val_loss: 1.3262 - val_accuracy: 0.1330\n",
      "Epoch 17/50\n",
      "107/107 [==============================] - 86s 802ms/step - loss: 1.0556 - accuracy: 0.1503 - val_loss: 1.2913 - val_accuracy: 0.1439\n",
      "Epoch 18/50\n",
      "107/107 [==============================] - 85s 793ms/step - loss: 1.0337 - accuracy: 0.1528 - val_loss: 1.2777 - val_accuracy: 0.1485\n",
      "Epoch 19/50\n",
      "107/107 [==============================] - 83s 781ms/step - loss: 1.0143 - accuracy: 0.1553 - val_loss: 1.2490 - val_accuracy: 0.1534\n",
      "Epoch 20/50\n",
      "107/107 [==============================] - 86s 806ms/step - loss: 0.9958 - accuracy: 0.1573 - val_loss: 1.2310 - val_accuracy: 0.1560\n",
      "Epoch 21/50\n",
      "107/107 [==============================] - 88s 824ms/step - loss: 0.9802 - accuracy: 0.1595 - val_loss: 1.2231 - val_accuracy: 0.1569\n",
      "Epoch 22/50\n",
      "107/107 [==============================] - 90s 846ms/step - loss: 0.9648 - accuracy: 0.1611 - val_loss: 1.2188 - val_accuracy: 0.1569\n",
      "Epoch 23/50\n",
      "107/107 [==============================] - 88s 822ms/step - loss: 0.9514 - accuracy: 0.1626 - val_loss: 1.2011 - val_accuracy: 0.1630\n",
      "Epoch 24/50\n",
      "107/107 [==============================] - 88s 823ms/step - loss: 0.9374 - accuracy: 0.1650 - val_loss: 1.1938 - val_accuracy: 0.1620\n",
      "Epoch 25/50\n",
      "107/107 [==============================] - 91s 850ms/step - loss: 0.9246 - accuracy: 0.1659 - val_loss: 1.1826 - val_accuracy: 0.1641\n",
      "Epoch 26/50\n",
      "107/107 [==============================] - 87s 811ms/step - loss: 0.9137 - accuracy: 0.1677 - val_loss: 1.1818 - val_accuracy: 0.1640\n",
      "Epoch 27/50\n",
      "107/107 [==============================] - 87s 812ms/step - loss: 0.9006 - accuracy: 0.1683 - val_loss: 1.1716 - val_accuracy: 0.1643\n",
      "Epoch 28/50\n",
      "107/107 [==============================] - 89s 833ms/step - loss: 0.8889 - accuracy: 0.1704 - val_loss: 1.1640 - val_accuracy: 0.1643\n",
      "Epoch 29/50\n",
      "107/107 [==============================] - 94s 880ms/step - loss: 0.8782 - accuracy: 0.1710 - val_loss: 1.1624 - val_accuracy: 0.1658\n",
      "Epoch 30/50\n",
      "107/107 [==============================] - 91s 847ms/step - loss: 0.8675 - accuracy: 0.1727 - val_loss: 1.1534 - val_accuracy: 0.1658\n",
      "Epoch 31/50\n",
      "107/107 [==============================] - 84s 782ms/step - loss: 0.8576 - accuracy: 0.1734 - val_loss: 1.1497 - val_accuracy: 0.1668\n",
      "Epoch 32/50\n",
      "107/107 [==============================] - 86s 803ms/step - loss: 0.8464 - accuracy: 0.1746 - val_loss: 1.1441 - val_accuracy: 0.1669\n",
      "Epoch 33/50\n",
      "107/107 [==============================] - 81s 755ms/step - loss: 0.8357 - accuracy: 0.1757 - val_loss: 1.1255 - val_accuracy: 0.1701\n",
      "Epoch 34/50\n",
      "107/107 [==============================] - 82s 766ms/step - loss: 0.8264 - accuracy: 0.1769 - val_loss: 1.1363 - val_accuracy: 0.1702\n",
      "Epoch 35/50\n",
      "107/107 [==============================] - 74s 689ms/step - loss: 0.8168 - accuracy: 0.1778 - val_loss: 1.1258 - val_accuracy: 0.1694\n",
      "Epoch 36/50\n",
      "107/107 [==============================] - 83s 781ms/step - loss: 0.8083 - accuracy: 0.1790 - val_loss: 1.1195 - val_accuracy: 0.1698\n",
      "Epoch 37/50\n",
      "107/107 [==============================] - 85s 792ms/step - loss: 0.7985 - accuracy: 0.1799 - val_loss: 1.1153 - val_accuracy: 0.1724\n",
      "Epoch 38/50\n",
      "107/107 [==============================] - 84s 783ms/step - loss: 0.7901 - accuracy: 0.1811 - val_loss: 1.1115 - val_accuracy: 0.1735\n",
      "Epoch 39/50\n",
      "107/107 [==============================] - 88s 824ms/step - loss: 0.7803 - accuracy: 0.1819 - val_loss: 1.1067 - val_accuracy: 0.1734\n",
      "Epoch 40/50\n",
      "107/107 [==============================] - 90s 845ms/step - loss: 0.7719 - accuracy: 0.1832 - val_loss: 1.0986 - val_accuracy: 0.1744\n",
      "Epoch 41/50\n",
      "107/107 [==============================] - 85s 796ms/step - loss: 0.7629 - accuracy: 0.1840 - val_loss: 1.1032 - val_accuracy: 0.1742\n",
      "Epoch 42/50\n",
      "107/107 [==============================] - 88s 826ms/step - loss: 0.7548 - accuracy: 0.1849 - val_loss: 1.1004 - val_accuracy: 0.1736\n",
      "Epoch 43/50\n",
      "107/107 [==============================] - 88s 828ms/step - loss: 0.7466 - accuracy: 0.1855 - val_loss: 1.0931 - val_accuracy: 0.1750\n",
      "Epoch 44/50\n",
      "107/107 [==============================] - 87s 815ms/step - loss: 0.7383 - accuracy: 0.1867 - val_loss: 1.0880 - val_accuracy: 0.1756\n",
      "Epoch 45/50\n",
      "107/107 [==============================] - 87s 810ms/step - loss: 0.7303 - accuracy: 0.1874 - val_loss: 1.0948 - val_accuracy: 0.1760\n",
      "Epoch 46/50\n",
      "107/107 [==============================] - 89s 836ms/step - loss: 0.7228 - accuracy: 0.1884 - val_loss: 1.0955 - val_accuracy: 0.1755\n",
      "Epoch 47/50\n",
      "107/107 [==============================] - 83s 778ms/step - loss: 0.7152 - accuracy: 0.1895 - val_loss: 1.0873 - val_accuracy: 0.1770\n",
      "Epoch 48/50\n",
      "107/107 [==============================] - 86s 805ms/step - loss: 0.7077 - accuracy: 0.1900 - val_loss: 1.0897 - val_accuracy: 0.1752\n",
      "Epoch 49/50\n",
      "107/107 [==============================] - 91s 845ms/step - loss: 0.7007 - accuracy: 0.1904 - val_loss: 1.0791 - val_accuracy: 0.1759\n",
      "Epoch 50/50\n",
      "107/107 [==============================] - 80s 747ms/step - loss: 0.6927 - accuracy: 0.1914 - val_loss: 1.0752 - val_accuracy: 0.1763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18812b134c0>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, we fit the model with the encoder and decoder inputs\n",
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, \n",
    "epochs = epochs, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.save('training_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = load_model('training_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is extracting the inputs, outputs, and hidden states of the encoder part of a pre-trained neural machine translation model.\n",
    "\n",
    "#This is the first input layer of the model, which corresponds to the source language sequence in a neural machine translation model.\n",
    "encoder_inputs = training_model.input[0]\n",
    "\n",
    "#These three variables correspond to the output and hidden states of the encoder part of the model. \n",
    "# encoder_outputs is the final output of the encoder, which is used as the input for the decoder part of the model. \n",
    "# state_h_enc and state_c_enc are the hidden states of the encoder, which are used to initialize the hidden states of \n",
    "# the decoder part of the model.\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "\n",
    "#This is a list that combines the hidden states state_h_enc and state_c_enc of the encoder. \n",
    "# These hidden states capture the context of the source language sequence and are used to initialize the hidden states of the decoder.\n",
    "encoder_states = [state_h_enc, state_c_enc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we build the model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code defines a new model, decoder_model, which is the decoder part of a neural machine translation model. \n",
    "# The code is defining the inputs and outputs of the decoder and how they are related.\n",
    "\n",
    "#These are two input layers, each with a shape of (latent_dim,). latent_dim is a hyperparameter that defines the number of \n",
    "# dimensions in the hidden state of the decoder. The two input layers represent the initial hidden states of the decoder.\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "\n",
    "#This is a list that combines the two input layers, decoder_state_input_hidden and decoder_state_input_cell.\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "\n",
    "#These are the output and hidden states of the decoder. decoder_lstm is an LSTM layer that takes the decoder_inputs and the \n",
    "# initial hidden states decoder_states_inputs as inputs, and produces the output and hidden states decoder_outputs, state_hidden, and state_cell.\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "#This is a list that combines the hidden states state_hidden and state_cell of the decoder.\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "\n",
    "#This line applies a dense layer, decoder_dense, to the output of the decoder, decoder_outputs.\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#This line creates the decoder_model using the Model class from the Keras library. The first argument is a list that \n",
    "# combines the decoder_inputs and decoder_states_inputs, which are the inputs of the decoder. The second argument is a list \n",
    "# that combines the decoder_outputs and decoder_states, which are the outputs of the decoder.\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(test_input):\n",
    "  # Encode the input as state vectors.\n",
    "  states_value = encoder_model.predict(test_input)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "  # Populate the first token of target sequence with the start token.\n",
    "  target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "\n",
    "  # Sampling loop for a batch of sequences\n",
    "  # (to simplify, here we assume a batch of size 1).\n",
    "  decoded_sentence = ''\n",
    "\n",
    "  stop_condition = False\n",
    "  while not stop_condition:\n",
    "    # Run the decoder model to get possible \n",
    "    # output tokens (with probabilities) & states\n",
    "    output_tokens, hidden_state, cell_state = decoder_model.predict(\n",
    "      [target_seq] + states_value)\n",
    "\n",
    "    # Choose token with highest probability\n",
    "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "    sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "    decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "    # Exit condition: either hit max length\n",
    "    # or find stop token.\n",
    "    if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "      stop_condition = True\n",
    "\n",
    "    # Update the target sequence (of length 1).\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "    # Update states\n",
    "    states_value = [hidden_state, cell_state]\n",
    "\n",
    "  return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "-\n",
      "Input sentence: I like fruit.\n",
      "Decoded sentence:  J'aime les les . <END>\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I like girls.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I like honey.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I like music.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I like opera.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I like sugar.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "-\n",
      "Input sentence: I like sushi.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "-\n",
      "Input sentence: I like these.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I like these.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "-\n",
      "Input sentence: I like women.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "-\n",
      "Input sentence: I like women.\n",
      "Decoded sentence:  J'aime les . <END>\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "-\n",
      "Input sentence: I liked that.\n",
      "Decoded sentence:  Je l'ai ai . <END>\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "-\n",
      "Input sentence: I live alone.\n",
      "Decoded sentence:  Je suis . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I live alone.\n",
      "Decoded sentence:  Je suis . <END>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "-\n",
      "Input sentence: I lost a bet.\n",
      "Decoded sentence:  J'ai un un . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "-\n",
      "Input sentence: I lost heart.\n",
      "Decoded sentence:  J'ai les ai . <END>\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "-\n",
      "Input sentence: I love birds.\n",
      "Decoded sentence:  J'adore les . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "-\n",
      "Input sentence: I love books.\n",
      "Decoded sentence:  J'adore les les .\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I love bread.\n",
      "Decoded sentence:  J'adore les . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "-\n",
      "Input sentence: I love games.\n",
      "Decoded sentence:  J'adore les . <END>\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I love jokes.\n",
      "Decoded sentence:  J'adore les les .\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "-\n",
      "Input sentence: I love jokes.\n",
      "Decoded sentence:  J'adore les les .\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "-\n",
      "Input sentence: I love music.\n",
      "Decoded sentence:  J'adore les . <END>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "-\n",
      "Input sentence: I love music.\n",
      "Decoded sentence:  J'adore les . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "-\n",
      "Input sentence: I love music.\n",
      "Decoded sentence:  J'adore les . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I love trips.\n",
      "Decoded sentence:  J'adore les les .\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I loved that.\n",
      "Decoded sentence:  Je . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I loved that.\n",
      "Decoded sentence:  Je . <END>\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "-\n",
      "Input sentence: I made a bet.\n",
      "Decoded sentence:  J'ai fait un un .\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "-\n",
      "Input sentence: I made it up.\n",
      "Decoded sentence:  Je l'ai ai . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "-\n",
      "Input sentence: I made plans.\n",
      "Decoded sentence:  J'ai préparé de .\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I made plans.\n",
      "Decoded sentence:  J'ai préparé de .\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I made these.\n",
      "Decoded sentence:  J'ai fait des . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "-\n",
      "Input sentence: I made these.\n",
      "Decoded sentence:  J'ai fait des . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I made these.\n",
      "Decoded sentence:  J'ai fait des . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I might stay.\n",
      "Decoded sentence:  Il s'est a fait .\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I missed you.\n",
      "Decoded sentence:  Tu vous vous . <END>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I must hurry.\n",
      "Decoded sentence:  Je me faut . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "-\n",
      "Input sentence: I must hurry.\n",
      "Decoded sentence:  Je me faut . <END>\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "-\n",
      "Input sentence: I must hurry.\n",
      "Decoded sentence:  Je me faut . <END>\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "-\n",
      "Input sentence: I must leave.\n",
      "Decoded sentence:  Il me faut . <END>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I must leave.\n",
      "Decoded sentence:  Il me faut . <END>\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I must study.\n",
      "Decoded sentence:  Il me faut . <END>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "-\n",
      "Input sentence: I need a bag.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need a cab.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "-\n",
      "Input sentence: I need a car.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need a hat.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I need a hug.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "-\n",
      "Input sentence: I need a job.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "-\n",
      "Input sentence: I need a job.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "-\n",
      "Input sentence: I need a job.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I need a map.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "-\n",
      "Input sentence: I need a map.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "-\n",
      "Input sentence: I need a pen.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need money.\n",
      "Decoded sentence:  J'ai besoin de . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "-\n",
      "Input sentence: I need paint.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need paint.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "-\n",
      "Input sentence: I need proof.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "-\n",
      "Input sentence: I need proof.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I need sleep.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need space.\n",
      "Decoded sentence:  J'ai besoin de . <END>\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I need sugar.\n",
      "Decoded sentence:  J'ai besoin de de\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "-\n",
      "Input sentence: I need to go.\n",
      "Decoded sentence:  Il me faut de . <END>\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need to go.\n",
      "Decoded sentence:  Il me faut de . <END>\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "-\n",
      "Input sentence: I need to go.\n",
      "Decoded sentence:  Il me faut de . <END>\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I need water.\n",
      "Decoded sentence:  J'ai besoin de . <END>\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "-\n",
      "Input sentence: I need water.\n",
      "Decoded sentence:  J'ai besoin de . <END>\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "-\n",
      "Input sentence: I never lose.\n",
      "Decoded sentence:  Je ne peux pas . <END>\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I often read.\n",
      "Decoded sentence:  Je l'ai ai . <END>\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "-\n",
      "Input sentence: I phoned him.\n",
      "Decoded sentence:  Je suis . <END>\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "-\n",
      "Input sentence: I play piano.\n",
      "Decoded sentence:  Je mange des ai .\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I play rugby.\n",
      "Decoded sentence:  Je mange ai ai . <END>\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "-\n",
      "Input sentence: I read a lot.\n",
      "Decoded sentence:  Je déteste un . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "-\n",
      "Input sentence: I read books.\n",
      "Decoded sentence:  Je les ai . <END>\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I rewrote it.\n",
      "Decoded sentence:  Je l'ai . <END>\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I rewrote it.\n",
      "Decoded sentence:  Je l'ai . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I said maybe.\n",
      "Decoded sentence:  J'ai les ai . <END>\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "-\n",
      "Input sentence: I see a book.\n",
      "Decoded sentence:  Je vois un un . <END>\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I see a lion.\n",
      "Decoded sentence:  Je vois un un . <END>\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I see a rose.\n",
      "Decoded sentence:  Je vois un un . <END>\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I see a star.\n",
      "Decoded sentence:  Je vois un un . <END>\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I sell shoes.\n",
      "Decoded sentence:  Je me ai ai . <END>\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I slept late.\n",
      "Decoded sentence:  J'ai été un . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "-\n",
      "Input sentence: I slept well.\n",
      "Decoded sentence:  J'ai essayé . <END>\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "-\n",
      "Input sentence: I smelled it.\n",
      "Decoded sentence:  Je l'ai suis . <END>\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "-\n",
      "Input sentence: I smelled it.\n",
      "Decoded sentence:  Je l'ai suis . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "-\n",
      "Input sentence: I started it.\n",
      "Decoded sentence:  Je l'ai . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "-\n",
      "Input sentence: I started it.\n",
      "Decoded sentence:  Je l'ai . <END>\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "-\n",
      "Input sentence: I started it.\n",
      "Decoded sentence:  Je l'ai . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "-\n",
      "Input sentence: I started it.\n",
      "Decoded sentence:  Je l'ai . <END>\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I still care.\n",
      "Decoded sentence:  Je me ai . <END>\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "-\n",
      "Input sentence: I suppose so.\n",
      "Decoded sentence:  Je suis en . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I swim a lot.\n",
      "Decoded sentence:  Je l'ai un . <END>\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I sympathize.\n",
      "Decoded sentence:  Je suis . <END>\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "-\n",
      "Input sentence: I teach here.\n",
      "Decoded sentence:  J'adore . <END>\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "-\n",
      "Input sentence: I thought so.\n",
      "Decoded sentence:  Je me suis ai . <END>\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "-\n",
      "Input sentence: I took a nap.\n",
      "Decoded sentence:  J'ai dispose un un\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "-\n",
      "Input sentence: I took risks.\n",
      "Decoded sentence:  J'ai été ai . <END>\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "-\n",
      "Input sentence: I tried both.\n",
      "Decoded sentence:  J'ai perdu les . <END>\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "-\n",
      "Input sentence: I tried hard.\n",
      "Decoded sentence:  J'ai les les . <END>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for seq_index in range(5000,5100):\n",
    "  test_input = encoder_input_data[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(test_input)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_docs[seq_index])\n",
    "  print('Decoded sentence:', decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f9455db543eabf0020c69a7ebcb16194ca9ff6bcf159dda4a4da7ba8d8694ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
