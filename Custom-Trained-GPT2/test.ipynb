{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "saved_data = torch.load('gpt2_model.pth')\n",
    "model = GPT2Model.from_pretrained(saved_data['model_name'], state_dict=saved_data['model_state_dict'])\n",
    "tokenizer = saved_data['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I am a generative hockey chatbot. I will finish your sentences for you. Try inputs like: The game was hard last night\n",
      "\n",
      "Bot: Sidney Crosby is the star of this summer's NHL Draft and he'll likely be selected in the first round. Crosby, who started his career at the University of North Dakota, has spent the past two years with the Blues and has shown great promise in his first two years\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import re\n",
    "\n",
    "saved_data = torch.load('gpt2_model.pth')\n",
    "model = GPT2LMHeadModel.from_pretrained(saved_data['model_name'], state_dict=saved_data['model_state_dict'])\n",
    "tokenizer = saved_data['tokenizer']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "exit_commands = ['exit', 'no', 'stop', 'quit']\n",
    "checkpoint=0\n",
    "# Get input from user\n",
    "print('Hello. I am a generative hockey chatbot. I will finish your sentences for you. Try inputs like: Sidney Crosby is\\n')\n",
    "while True:\n",
    "    \n",
    "    prompt = input(\"You: \")\n",
    "\n",
    "    \n",
    "    if prompt.lower() in exit_commands:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "   \n",
    "        # Encode the prompt\n",
    "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(encoded_prompt.shape, dtype=torch.long)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "        # Generate a response\n",
    "    output = model.generate(encoded_prompt, temperature=0.7, do_sample=True, attention_mask=attention_mask, pad_token_id=pad_token_id,max_length=70)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response = response.rsplit(\".\", 1)[0]\n",
    "\n",
    "    tokens = response.split()\n",
    "    num_repeats = 0\n",
    "    for i in range(len(tokens) - 1):\n",
    "        if tokens[i] == tokens[i+1]:\n",
    "            num_repeats += 1\n",
    "\n",
    "    # Cut off the response if it contains too many repexit\n",
    "    # eated phrases\n",
    "    max_repeats = 3\n",
    "    if num_repeats > max_repeats:\n",
    "        pattern = r'\\b(' + re.escape(tokens[i]) + r')\\b(\\W+\\1\\b)+'\n",
    "        response = re.sub(pattern, r'\\1', response)\n",
    "        response = response[:response.rfind('.')+1]\n",
    "    \n",
    "    response = response.replace('\"', '')\n",
    "\n",
    "    if '\\n\\n' in response:\n",
    "        empty_line_index = response.index('\\n\\n')\n",
    "        response = response[:empty_line_index]\n",
    "        response = response.strip()\n",
    "        response += '.'\n",
    "        \n",
    "    print(\"Bot:\", response)\n",
    "  \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b9ade5438847fe1fab0d0df983923093012075350f03489151d89fc9faf1d4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
